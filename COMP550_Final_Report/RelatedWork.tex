\section{Related Work}
The segmentation of Chinese text has been extensively researched due to the language's absence of explicit word boundaries. The study~\citep{xue-2003-chinese} have pointed out that the character-level tokenization might not fully capture the semantic meaning of phrases, as individual Chinese characters often convey less distinct information than full words. \citep{zhou-2003-chunking} discussed how word-level tokenization, which aligns more closely with nature of Chinese, could enhance semantic understanding of models in Chinese NLP tasks. Recent studies, such as~\citep{sun2022word}, employ the word-level tokenization tool \textit{Jieba}, which leverages dictionary-based methods and Hidden Markov Models to improve segmentation efficiency. Moreover, the research~\citep{si-etal-2023-sub} has shown that the phonetic-based approach of pinyin-level tokenization can address homophony and improve computational efficiency, but may sacrifice semantic depth due to its focus on pronunciation. In this study, we investigated the impact of three distinct tokenization schemes, as well as the application of bigrams feature extraction in character-level tokenization, for sentiment analysis and text classification tasks. To the best of our knowledge, this is the first work to comprehensively compare these four approaches in the domain of Chinese text analysis. 

The word2vec embedding introduced by~\citep{mikolov2013efficient}, revolutionized the field of NLP by transforming words into numerical vectors that capture semantic and syntactic information. The work~\citep{li2018ana} has proven the effectiveness of the word2vec embedding. These embeddings, when utilized with LSTM models, which are known for their capacity to capture the sequential nature of text, provide robust semantic encoding~\citep{yuan2023}.

Moreover, bidirectional LSTM (BiLSTM) models extend this capability by processing data in both directions with two separate hidden layers, which are then fed forwards to the same output layer~\citep{graves2005}. Recent researches, such as~\citep{yuan2023}, have also explored the integration of an attention mechanism with BiLSTM to improve feature extraction. In this project, we investigated the effectiveness of standard LSTM, BiLSTM, and BiLSTM enhanced with an attention mechanism. Additionally, we utilized LR for analyzing Chinese texts, a method not extensively adopted in previous studies, and assessed its performance against LSTM-based models.
