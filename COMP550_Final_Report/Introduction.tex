\section{Introduction}
Chinese Natural Language Processing (NLP) tasks present distinct challenges, primarily due to the absence of explicit word boundaries inherent to the language. This absence renders word segmentation a critical and complex task. In this investigation, we probe into various approaches of Chinese text tokenization. Character-level tokenization is a straightforward method which does not need extensive linguistic knowledge but may miss semantic meanings of compound words. In contrast, word-level tokenization aligns more naturally with Chinese linguistic patterns; however, it contends with the challenge of ambiguous word boundaries. To tackle this complexity, several tools such as \textit{HanLP}, \textit{THULAC}, \textit{SnowNLP}, and \textit{Jieba} have been developed, each designed to adeptly deal with the uniqueness of the Chinese language, such as its abundance of homophones. 

Sub-character tokenization schemes, particularly those utilizing phonetic representations like pinyin, offer an alternative approach. As introduced by~\citep{si-etal-2023-sub}, these schemes not only increase computational efficiency by reducing input dimension of the embedding layer but also are robust to homophone typos by embedding them to the same vector representation. However, a potential drawback of the pinyin-level tokenization scheme is their tendency to neglect the semantic context because of their emphasis on phonetics. This study aims to assess the impact of these tokenization methods on the performance of NLP models in Chinese text analysis tasks.
 
Beyond tokenization, word2vec \citep{mikolov2013efficient} embedding plays a crucial role in analyzing Chinese context. It transforms words or characters into vectors that encapsulate semantic patterns, allowing models to discern linguistic contexts and relationships. In Chinese NLP, the ability of word2vec to capture linguistic regularities and semantic relationships in a dense vector space significantly enhances tasks like similarity detection and sentiment analysis~\citep{li2018ana}. 

Our work applies different models to evaluate the influence of different tokenization strategies on two common NLP tasks: sentiment analysis and text classification. Sentiment analysis is integral for extracting the emotional tone from text, which is vital for understanding consumer behavior in online reviews. Text classification, on the other hand, is essential for systematically categorizing text, crucial for effective information retrieval. Given the importance of contextual understanding in Chinese, models capable of capturing long-term dependencies, such as LSTM, are particularly useful for these tasks.

In this project, Logistic Regression (LR) and LSTM models are employed for exploring the impact of various tokenization schemes, including character-level, word-level, and pinyin-level, along with the exploration of bigrams in character-level tokenization for text classification and sentiment analysis tasks. Moreover, the word2vec embedding is also introduced to LSTMs to evaluate its ability in capturing linguistic regularities. Taking into account of these confounding factors in model selection, the primary objective of this study is to determine the optimal level of token granularity for sentiment analysis and text classification based on model performance as measured by accuracy and F1 score. Experiments are conducted on the \textit{onlineshopping10cats} dataset, which is widely used in Chinese text analysis~\citep{sun2022word, graves2005}.

The contributions of this work are as follows: i. Systematically explored the effect of various Chinese tokenization methods for linear and neural models in sentiment analysis and text classification tasks ii. Explored the effectiveness of word2vec embeddings for both character and word tokens in Chinese text analysis. iii. Investigated the performance of tone-less pinyin tokenization for sentiment analysis and text classification tasks.